import requests
import re
import time
import json
from abc import ABC, abstractmethod

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#   TrendRadar æ ¸å¿ƒé…ç½® (ä¸¥æ ¼éµå¾ª YAML æ ¼å¼é€»è¾‘)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CONFIG = {
    # ===============================================================
    # 1. åŸºç¡€è®¾ç½®
    # ===============================================================
    "app": {
        "timezone": "Asia/Shanghai",
        "show_version_update": True
    },

    # ===============================================================
    # 2. æ•°æ®æº - çƒ­æ¦œå¹³å°
    # ===============================================================
    "platforms": {
        "enabled": True,
        "sources": [
            {"id": "douyin", "name": "æŠ–éŸ³", "enabled": True},
            {"id": "bilibili", "name": "Bilibili", "enabled": True},
            {"id": "tieba", "name": "è´´å§", "enabled": True},
            {"id": "wechat", "name": "å¾®ä¿¡", "enabled": True},     # æ–°å¢
            {"id": "xiaohongshu", "name": "å°çº¢ä¹¦", "enabled": True} # æ–°å¢
        ]
    },

    # ===============================================================
    # [æ–°å¢] è¿‡æ»¤è§„åˆ™é…ç½® (ç”¨äºå®ç°ä½ çš„ç‰¹å®šæŠ“å–éœ€æ±‚)
    # ===============================================================
    "filter_rules": {
        # å¼ºåˆ¶æŠ“å–çš„ç²¾å‡†å…³é”®è¯ (ä¼˜å…ˆçº§æœ€é«˜)
        "must_include_keywords": [
            "å‰‘ç½‘3", "å‰‘ä¸‰", "ç‡•äº‘åå…­å£°", "é€†æ°´å¯’"
        ],
        # ç›®æ ‡åˆ†ç±» (ç”¨äºé€šç”¨çƒ­ç‚¹ç­›é€‰)
        "categories": {
            "gaming": {
                "name": "ğŸ® æ¸¸æˆåœˆ",
                "keywords": ["æ¸¸æˆ", "ç”µç«", "Steam", "æ‰‹æ¸¸", "å…¬æµ‹", "ç‰ˆæœ¬", "æ”»ç•¥", "é»‘ç¥è¯", "ç±³å“ˆæ¸¸", "è…¾è®¯æ¸¸æˆ"]
            },
            "entertainment": {
                "name": "ğŸ¬ æ–‡å¨±åœˆ",
                "keywords": ["æ˜æ˜Ÿ", "ç”µå½±", "ç”µè§†å‰§", "ç»¼è‰º", "å¡Œæˆ¿", "å®˜å®£", "æ¼”å”±ä¼š", "çƒ­æœ"]
            },
            "funny": {
                "name": "ğŸ¤£ æç¬‘/æ²™é›•",
                "keywords": ["æç¬‘", "äººç±»æ»¡çº§", "ç¦»è°±", "å“ˆå“ˆ", "æ•´æ´»", "ååœºé¢", "é¬¼ç•œ"]
            }
        }
    },

    # ===============================================================
    # 11. é«˜çº§è®¾ç½® (ç”¨äºçˆ¬è™«å‚æ•°)
    # ===============================================================
    "advanced": {
        "crawler": {
            "request_interval": 2000,
            "headers": {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            },
            # âš ï¸ å°çº¢ä¹¦å’Œå¾®ä¿¡é€šå¸¸éœ€è¦ Cookie æ‰èƒ½ç¨³å®šæŠ“å–ï¼Œæ­¤å¤„é¢„ç•™é…ç½®
            "cookies": {
                "xiaohongshu": "", 
                "wechat": ""
            }
        }
    }
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#   ä»£ç é€»è¾‘å®ç°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class HotItem:
    def __init__(self, title, url, platform, category="æœªåˆ†ç±»", is_target=False):
        self.title = title
        self.url = url
        self.platform = platform
        self.category = category
        self.is_target = is_target # æ˜¯å¦å‘½ä¸­äº†æŒ‡å®šçš„æ¸¸æˆå…³é”®è¯(å‰‘ç½‘3ç­‰)

    def __repr__(self):
        mark = "â˜…" if self.is_target else "â€¢"
        return f"{mark} [{self.platform}][{self.category}] {self.title}"

class BaseScraper(ABC):
    def __init__(self, config):
        self.config = config
        self.headers = config["advanced"]["crawler"]["headers"]
    
    @abstractmethod
    def scrape(self):
        pass

# 1. Bilibili çˆ¬è™« (APIç‰ˆ)
class BilibiliScraper(BaseScraper):
    def scrape(self):
        results = []
        # rid: 4=æ¸¸æˆ, 5=å¨±ä¹, 138=æç¬‘
        target_rids = {4: "æ¸¸æˆ", 5: "æ–‡å¨±", 138: "æç¬‘"}
        api_url = "https://api.bilibili.com/x/web-interface/ranking/v2?rid={}"
        
        for rid, cat_name in target_rids.items():
            try:
                resp = requests.get(api_url.format(rid), headers=self.headers, timeout=10)
                data = resp.json()
                if data['code'] == 0:
                    for item in data['data']['list'][:10]:
                        results.append(HotItem(
                            title=item['title'],
                            url=f"https://www.bilibili.com/video/{item['bvid']}",
                            platform="Bilibili",
                            category=cat_name
                        ))
                time.sleep(1)
            except Exception as e:
                print(f"[!] Bilibili {cat_name} error: {e}")
        return results

# 2. æŠ–éŸ³çˆ¬è™« (APIç‰ˆ)
class DouyinScraper(BaseScraper):
    def scrape(self):
        results = []
        url = "https://www.douyin.com/aweme/v1/web/hot/search/list/"
        try:
            # å°è¯•è·å–ï¼Œå¦‚æœå¤±è´¥(é£æ§)åˆ™è·³è¿‡
            resp = requests.get(url, headers=self.headers, timeout=10)
            data = resp.json()
            if 'data' in data and 'word_list' in data['data']:
                for item in data['data']['word_list'][:20]:
                    results.append(HotItem(
                        title=item['word'],
                        url=f"https://www.douyin.com/search/{item['word']}",
                        platform="æŠ–éŸ³",
                        category="å¾…æ¸…æ´—" # æŠ–éŸ³çƒ­æ¦œæ··åˆï¼Œéœ€åç»­æ¸…æ´—
                    ))
        except:
            pass # å¿½ç•¥æŠ–éŸ³é£æ§é”™è¯¯
        return results

# 3. è´´å§çˆ¬è™« (HTMLç‰ˆ)
class TiebaScraper(BaseScraper):
    def scrape(self):
        results = []
        url = "http://tieba.baidu.com/hottopic/browse/topicList"
        try:
            resp = requests.get(url, headers=self.headers, timeout=10)
            # ç®€å•æ­£åˆ™æå–
            titles = re.findall(r'"topic_name":"(.*?)"', resp.text)
            urls = re.findall(r'"topic_url":"(.*?)"', resp.text)
            for i in range(min(len(titles), 20)):
                try:
                    t = titles[i].encode('utf-8').decode('unicode_escape')
                    u = urls[i].replace(r'\/', '/')
                    results.append(HotItem(t, u, "è´´å§", "å¾…æ¸…æ´—"))
                except: continue
        except Exception as e:
            print(f"[!] Tieba error: {e}")
        return results

# 4. å¾®ä¿¡çˆ¬è™« (Sogouæœç´¢ç‰ˆ - æ¨¡æ‹Ÿ)
class WechatScraper(BaseScraper):
    def scrape(self):
        # çœŸå®å¾®ä¿¡æŠ“å–æéš¾ï¼Œæ­¤å¤„æ¼”ç¤ºé€šè¿‡æœç‹—å¾®ä¿¡çƒ­è¯æ¥å£é€»è¾‘
        # å®é™…ç”Ÿäº§ç¯å¢ƒé€šå¸¸éœ€è¦ Playwright
        results = []
        url = "https://weixin.sogou.com/"
        try:
            resp = requests.get(url, headers=self.headers, timeout=10)
            # æå–å³ä¾§çƒ­è¯æ¦œ (æ¨¡æ‹Ÿæ­£åˆ™)
            hot_words = re.findall(r'title="(.*?)"', resp.text) # ç®€åŒ–é€»è¾‘
            for word in hot_words[:10]:
                 if len(word) > 4: # ç®€å•è¿‡æ»¤æ‚è®¯
                    results.append(HotItem(
                        title=word,
                        url=f"https://weixin.sogou.com/weixin?type=2&query={word}",
                        platform="å¾®ä¿¡",
                        category="å¾…æ¸…æ´—"
                    ))
        except: pass
        return results

# 5. å°çº¢ä¹¦çˆ¬è™« (æ¨¡æ‹Ÿç‰ˆ)
class XiaohongshuScraper(BaseScraper):
    def scrape(self):
        # å°çº¢ä¹¦å¿…é¡»éªŒè¯ Cookieï¼Œå¦åˆ™æ— æ³•é€šè¿‡ Requests è·å–æ•°æ®
        # è¿™é‡Œè¿”å›ä¸€ä¸ªæç¤ºæ€§æ•°æ®æˆ–åŸºäº Cookie çš„å°è¯•
        results = []
        cookie = self.config['advanced']['crawler']['cookies']['xiaohongshu']
        if not cookie:
            # å¦‚æœæ²¡é… Cookieï¼Œç”Ÿæˆå‡ æ¡ç‰¹å®šæ¸¸æˆçš„æœç´¢é“¾æ¥ä½œä¸ºå ä½ï¼Œç¡®ä¿ä¸æ¼æ‰é‡ç‚¹
            targets = self.config['filter_rules']['must_include_keywords']
            for t in targets:
                results.append(HotItem(
                    title=f"å°çº¢ä¹¦æœç´¢: {t}",
                    url=f"https://www.xiaohongshu.com/search_result?keyword={t}",
                    platform="å°çº¢ä¹¦",
                    category="ä¸»åŠ¨ç›‘æ§",
                    is_target=True
                ))
            return results
        
        # å¦‚æœæœ‰ Cookieï¼Œå¯å°è¯•è°ƒç”¨ API (ä¼ªä»£ç )
        # url = "https://edith.xiaohongshu.com/api/sns/web/v1/search/notes"
        return results

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#   æ•°æ®æ¸…æ´—ä¸æ ¸å¿ƒé€»è¾‘
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TrendProcessor:
    def __init__(self, config):
        self.rules = config['filter_rules']
        
    def process(self, raw_items):
        final_items = []
        target_keywords = self.rules['must_include_keywords']
        categories = self.rules['categories']

        for item in raw_items:
            # 1. ä¼˜å…ˆåŒ¹é…æŒ‡å®šæ¸¸æˆ (å‰‘ç½‘3, ç‡•äº‘, é€†æ°´å¯’)
            # åªè¦åŒ…å«è¿™äº›è¯ï¼Œæ— è§†åˆ†ç±»ï¼Œç›´æ¥å…¥é€‰ï¼Œå¹¶æ ‡è®°ä¸ºé«˜äº®
            for target in target_keywords:
                if target in item.title:
                    item.is_target = True
                    item.category = "ğŸ¯ é‡ç‚¹ç›‘æ§"
                    final_items.append(item)
                    break # å‘½ä¸­åè·³å‡ºå¾ªç¯
            
            if item.is_target:
                continue

            # 2. å¦‚æœæ²¡å‘½ä¸­æŒ‡å®šæ¸¸æˆï¼Œåˆ™åŒ¹é…é€šç”¨åˆ†ç±» (æ¸¸æˆ/æ–‡å¨±/æç¬‘)
            if item.category != "å¾…æ¸…æ´—" and item.category != "æœªåˆ†ç±»":
                # å·²ç»æ˜¯ Bç«™ åˆ†å¥½ç±»çš„
                final_items.append(item)
                continue

            # 3. å¯¹æ··åˆæ¥æºè¿›è¡Œå…³é”®è¯åˆ†ç±»
            for cat_key, cat_val in categories.items():
                match = False
                for keyword in cat_val['keywords']:
                    if keyword in item.title:
                        item.category = cat_val['name']
                        final_items.append(item)
                        match = True
                        break
                if match: break

        return final_items

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#   ä¸»ç¨‹åºå…¥å£
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_trend_radar():
    print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print("TrendRadar - æ ¸å¿ƒçƒ­ç‚¹ç›‘æ§å¯åŠ¨")
    print("ç›‘æ§é‡ç‚¹: å‰‘ç½‘3 / ç‡•äº‘åå…­å£° / é€†æ°´å¯’")
    print("ç›‘æ§åˆ†ç±»: æ¸¸æˆ / æ–‡å¨± / æç¬‘")
    print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    # 1. å®ä¾‹åŒ–çˆ¬è™«
    scrapers = [
        BilibiliScraper(CONFIG),
        DouyinScraper(CONFIG),
        TiebaScraper(CONFIG),
        WechatScraper(CONFIG),
        XiaohongshuScraper(CONFIG)
    ]

    all_data = []

    # 2. æ‰§è¡ŒæŠ“å–
    for scraper in scrapers:
        p_name = scraper.__class__.__name__.replace("Scraper", "")
        if next((s for s in CONFIG['platforms']['sources'] if s['id'] == p_name.lower() and s['enabled']), None):
            print(f"ğŸ“¡ æ­£åœ¨æŠ“å– {p_name} ...")
            items = scraper.scrape()
            all_data.extend(items)
            time.sleep(1) 

    # 3. æ•°æ®æ¸…æ´—
    processor = TrendProcessor(CONFIG)
    final_data = processor.process(all_data)

    # 4. ç»“æœå±•ç¤º
    print(f"\nâœ… æŠ“å–å®Œæˆï¼Œç­›é€‰å‡º {len(final_data)} æ¡æœ‰æ•ˆä¿¡æ¯ï¼š\n")
    
    # ä¼˜å…ˆå±•ç¤ºé‡ç‚¹ç›‘æ§
    targets = [x for x in final_data if x.is_target]
    others = [x for x in final_data if not x.is_target]

    if targets:
        print("ğŸ¯ ã€é‡ç‚¹ç›‘æ§é¡¹ç›®ã€‘")
        for item in targets:
            print(f"   {item}")
            print(f"   â””â”€ {item.url}")
        print("-" * 50)

    # æŒ‰åˆ†ç±»å±•ç¤ºå…¶ä»–
    current_cat = ""
    others.sort(key=lambda x: x.category)
    for item in others:
        if item.category != current_cat:
            print(f"\nğŸ“‚ ã€{item.category}ã€‘")
            current_cat = item.category
        print(f"   â€¢ [{item.platform}] {item.title}")
        print(f"     â””â”€ {item.url}")

if __name__ == "__main__":
    run_trend_radar()
